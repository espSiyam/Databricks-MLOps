{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c59f3d11-d42d-43e7-8d4d-64bbbecc9fc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.ml.feature import StringIndexer, Word2Vec, VectorAssembler\n",
    "from pyspark.ml.linalg import DenseVector, VectorUDT\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06d9412d-d1e2-4094-b825-6336318dadef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "telco_path = \"sdd_dev.sohag_test.telco_customer_churn\"\n",
    "telco_df = spark.read.table(telco_path)\n",
    "display(telco_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc673cf-ad29-4a83-a849-46a6147440c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Processing the dataset\n",
    "\n",
    "## Handing missing values\n",
    "* NULL values might be stored as \"null\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "599e0001-f3a9-4c51-baf1-2ccb128c8d94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bad_vals = [\"null\", \"\"]          # add more literals if you find them\n",
    "for c in telco_df.columns:\n",
    "    telco_df = telco_df.withColumn(\n",
    "        c,\n",
    "        when(trim(col(c)).isin(bad_vals), None)      # \"null\", \"\", \"   \"\n",
    "        .otherwise(col(c))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e2b3f8c-a955-431c-8928-e0c3376e2d54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2️⃣  Cast columns that should be numeric ------------------------------------\n",
    "#    – add any other numeric columns you have\n",
    "numeric_cols = [\"SeniorCitizen\", \"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
    "for c in numeric_cols:\n",
    "    telco_df = telco_df.withColumn(c, col(c).cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b380d4f-2cb2-42ff-9e20-461ca2dace08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # 3️⃣  Optional: drop any rows that still contain nulls -----------------------\n",
    "# telco_df = telco_df.dropna() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b27242ac-4d52-4823-b7f3-55ed976b93b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Converting datatype\n",
    "* SeniorCitizen should be Boolean.\n",
    "* TotalCharges should be double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f53b0b1-c9d7-4474-9ba5-56a9ad15059d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "telco_df = telco_df.withColumn(\"SeniorCitizen\", when(col(\"SeniorCitizen\")==1, True).otherwise(False))\n",
    "\n",
    "display(telco_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cf51084-ed08-45d8-9ef0-c031ad55dd31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "telco_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1501b8bd-4533-4817-9f83-3bd7c31a8f26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Splitting the dataset into trianing and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "141934a7-cf6e-4c54-8611-2396c9283269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = telco_df.randomSplit([.8, .2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ca602d8-8a42-40fc-b8d3-cee8d1e447d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Transforming the dataset\n",
    "* Coverting Integer and Boolena columns to double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5bddf09-98e0-49c0-ab16-2d5f611ddf1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get a list of integer and boolean columns\n",
    "integer_cols = [column.name for column in train_df.schema.fields if (column.dataType == IntegerType() or column.dataType == BooleanType())]\n",
    "print(integer_cols)\n",
    "\n",
    "for column in integer_cols:\n",
    "    train_df = train_df.withColumn(column, col(column).cast(\"double\"))\n",
    "    test_df = test_df.withColumn(column, col(column).cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "559783a2-09a3-4a69-924b-9bc0977bf5a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get a list of numeric columns\n",
    "num_cols = [c.name for c in train_df.schema.fields if c.dataType == DoubleType()]\n",
    "print(num_cols)\n",
    "\n",
    "# Dictionary of {column: missing_count} for columns with missing values\n",
    "num_missing_cols = [\n",
    "    c for c in num_cols\n",
    "    if train_df.filter(col(c).isNull()).count() > 0\n",
    "]\n",
    "print(num_missing_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98aa6403-f501-4312-8b31-7e4c8a990fd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Identify string columns\n",
    "string_cols = [c.name for c in train_df.schema.fields if isinstance(c.dataType, StringType)]\n",
    "\n",
    "# 2. Count missing values in string columns\n",
    "string_missing_values_logic = [\n",
    "    count(when(col(column).isNull(), column)).alias(column) for column in string_cols\n",
    "]\n",
    "row_dict_string = train_df.select(string_missing_values_logic).first().asDict()\n",
    "string_missing_cols = [column for column in row_dict_string if row_dict_string[column] > 0]\n",
    "\n",
    "print(f\"String columns with missing values: {string_missing_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e95f38d-a96a-40b9-967f-75b6e8d4fdd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Creating a Feature Engineering Pipeline (Spark ML pipeline)\n",
    "* Generate embeffing for categoriacal features\n",
    "* Handling missing values\n",
    "* Standardizing numerical feature\n",
    "* Combining features into a Final vector\n",
    "* Encapsulating steps into a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee450e86-c82f-43c6-9e5f-3d105d3c6a6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "def generate_categorical_embeddings(df, categorical_cols, vector_size = 5):\n",
    "    \"\"\"\n",
    "    Generate embeddings for categorical columns using Word2Vec\n",
    "\n",
    "    Parameters:\n",
    "    df (pyspark.sql.DataFrame): Input DataFrame\n",
    "    categorical_cols (list): List of categorical columns to generate embeddings for\n",
    "    vector_size (int): Size of the embedding vector\n",
    "\n",
    "    Returns:\n",
    "    pyspark.sql.DataFrame: DataFrame with embeddings for categorical columns\n",
    "    \"\"\"\n",
    "\n",
    "    # Replace NULL categorical values with \"unknown\"\n",
    "    for col_name in categorical_cols:\n",
    "        df = df.withColumn(col_name, when(col(col_name).isNull(), \"unknown\").otherwise(col(col_name)))\n",
    "\n",
    "    # Combine all categoical columns into a single text column for Word2Vec\n",
    "    df = df.withColumn(\"categorical_sequence\", concat_ws(\" \", *categorical_cols))\n",
    "\n",
    "    # Tokenize categorical data\n",
    "    df = df.withColumn(\"categorical_tokens\", split(col(\"categorical_sequence\"), \" \"))\n",
    "\n",
    "    # Infer Word2Vec model for categorical columns\n",
    "    word2vec = Word2Vec(vectorSize=vector_size, inputCol=\"categorical_tokens\", outputCol=\"categorical_struct\")\n",
    "    model = word2vec.fit(df)\n",
    "\n",
    "    # Generate embeddings for categorical columns\n",
    "    df = model.transform(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d13ef8a-2c16-40f1-be89-be4b99e23578",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Applying Embedding to Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "855f7a94-d8c0-4da7-b798-1853f3caae66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical_columns = [\"gender\", \"Partner\", \"InternetService\", \"Contract\", \"PaperlessBilling\", \"PaymentMethod\", \"Churn\"]\n",
    "\n",
    "# Generate embeddings for categorical columns\n",
    "train_df = generate_categorical_embeddings(train_df, categorical_columns)\n",
    "test_df = generate_categorical_embeddings(test_df, categorical_columns)\n",
    "\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "476d544a-5e5e-40be-aaa9-669465e18905",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract the 'value' field from the word3vec struct\n",
    "DenseVector_udf = F.udf(lambda v: DenseVector(v.values) if v else DenseVector([0.0] * 5), VectorUDT())\n",
    "\n",
    "# Convert embeddings into DenseVectors\n",
    "for col_name in categorical_columns:\n",
    "  train_df = train_df.withColumn(col_name + \"_embeddings\", DenseVector_udf(F.col(\"categorical_struct\")))\n",
    "  test_df = test_df.withColumn(col_name + \"_embeddings\", DenseVector_udf(F.col(\"categorical_struct\")))\n",
    "\n",
    "# Drop unnecessary columns after embeddings are extracted\n",
    "train_df = train_df.drop(\"categorical_sequence\", \"categorical_tokens\", \"categorical_struct\")\n",
    "test_df = test_df.drop(\"categorical_sequence\", \"categorical_tokens\", \"categorical_struct\")\n",
    "\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5633a05e-35ae-4023-8e72-b32e652e89ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show a sample of embedded categorical features\n",
    "train_df.select(\"PaymentMethod\", \"PaymentMethod_embeddings\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0d3470d-10d6-4603-9eae-7071816d5560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Engineering and Pipeline Initialization\n",
    "* Handle missing val in Numerical Column.\n",
    "* Standardize Numerical Feature\n",
    "* Assemble the Final Feature Vector\n",
    "* Initializing the Spark ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38a714ce-3cfb-4c62-bbbc-0789c4acfc5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define numerical column for imputation\n",
    "numerical_cols = [\"SeniorCitizen\", \"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
    "\n",
    "# Impute missing numerical feature\n",
    "imputer = Imputer(inputCols=numerical_cols, outputCols=[col + \"_imputed\" for col in numerical_cols])\n",
    "\n",
    "# Assemble numerical columns into a single vector\n",
    "numerical_assembler = VectorAssembler(\n",
    "    inputCols=[col + \"_imputed\" for col in numerical_cols], \n",
    "    outputCol=\"numerical_assembled\"\n",
    ")\n",
    "\n",
    "# Scale numerical featured to standardize values\n",
    "numerical_Scaler = StandardScaler(inputCol=\"numerical_assembled\", outputCol= \"numerical_scaled\")\n",
    "\n",
    "# Assemble all features (numerical + categorical embeddings) into a single feature vector\n",
    "feature_cols = [\"numerical_scaled\"] + [col + \"_embeddings\" for col in categorical_columns]\n",
    "vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"all_features\")\n",
    "\n",
    "# Define the sequence of the transformations\n",
    "stages_list = [imputer, numerical_assembler, numerical_Scaler, vector_assembler]\n",
    "\n",
    "# Initiate the pipeline\n",
    "pipeline = Pipeline(stages = stages_list)\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "017dc36d-5632-42de-9331-a17fe26e0c14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transform both training_df and test_df \n",
    "train_transformed_df = pipeline_model.transform(train_df)\n",
    "test_transformed_df = pipeline_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "174cfd99-c8f2-4459-806d-4f51f372801c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753129470194}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show transformed features\n",
    "display(train_transformed_df.select('all_features'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48216cf8-8776-490b-9e44-245faf09315b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Save the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fccfc570-445d-409f-a422-c8d8d6f274de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_path = \"/Workspace/Shared/artifact_wheels/spark_pipelines\"\n",
    "pipeline_model.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc9e32a1-abe3-4e9a-a96e-2f7f2882d1a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Load and use saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d994925d-2979-43cf-8c8f-7c980d03530a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "loaded_pipeline = PipelineModel.load(model_path)\n",
    "\n",
    "# Show pipeline stages\n",
    "loaded_pipeline.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46a6fb63-9126-4122-b223-c89f9769a52c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the loaded pipeline to transofrm the test dataset\n",
    "test_transformed_df = loaded_pipeline.transform(test_df)\n",
    "display(test_transformed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb756edd-8ab4-4157-ba7d-b6598e69fb7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pipeline - Feature Engineering and Embedding",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
