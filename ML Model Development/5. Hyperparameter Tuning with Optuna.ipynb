{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "748e787a-2757-43b8-9653-c8732f10e484",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Hyperparameter Tuning with Optuna\n",
    "\n",
    "In this hands-on demo, you will learn how to leverage **Optuna**, a powerful optimization library, for efficient model tuning. We'll guide you through the process of performing **hyperparameter optimization**, demonstrating how to define the search space, objective function, and algorithm selection. Throughout the demo, you will utilize _MLflow_ to seamlessly track the model tuning process, capturing essential information such as hyperparameters, metrics, and intermediate results. By the end of the session, you will not only grasp the principles of hyperparameter optimization but also be proficient in finding the best-tuned model using various methods such as the **MLflow API** and **MLflow UI**.\n",
    "\n",
    "By integrating Optuna and MLflow, you can efficiently optimize hyperparameters and maintain comprehensive records of your machine learning experiments, facilitating reproducibility and collaborative research.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "**By the end of this demo, you will be able to:**\n",
    "\n",
    "- Perform hyperparameter optimization using Optuna.\n",
    "- Track the model tuning process with MLflow.\n",
    "- Query previous runs from an experiment using the `MLflowClient`.\n",
    "- Review an MLflow Experiment for visualizing results and selecting the best run.\n",
    "- Read in the best model, make a prediction, and register the model to Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1f19506-9698-45c6-86da-56f78a9c42ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a407e2a0-1c2e-47a3-bd2d-d81407983c03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Installing the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96af9d2b-f3a9-484f-81ef-4e7570a9ca8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qq optuna\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e69ea685-f709-4a6c-9825-6fc3a0d65477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from mlflow.models.signature import infer_signature\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4055452-8bd4-4f17-859d-e0c1509f2859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"sdd_dev.sohag_test.diabetes_binary_health_indicators_brfss_2015\"\n",
    "diabetes_dataset = spark.read.table(table_name)\n",
    "diabetes_pd = diabetes_dataset.toPandas()\n",
    "diabetes_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a74708e-4a61-4254-9b34-45bf0598838e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spliting Train/ Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59cd4fea-b27e-4697-913e-4867dce67364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"We have {diabetes_pd.shape[0]} records in our source dataset\")\n",
    "\n",
    "# split target variable into its own dataset\n",
    "target_col = \"Diabetes_binary\"\n",
    "X_all = diabetes_pd.drop(labels=target_col, axis=1)\n",
    "y_all = diabetes_pd[target_col]\n",
    "\n",
    "# test / train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, train_size=0.95, random_state=42)\n",
    "\n",
    "y_train = y_train.astype(float)\n",
    "y_test = y_test.astype(float)\n",
    "\n",
    "print(f\"We have {X_train.shape[0]} records in our training dataset\")\n",
    "print(f\"We have {X_test.shape[0]} records in our test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7353e924-320e-4744-820f-9987ed1d48f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Hyperparameter Tuning with Optuna and MLflow\n",
    "\n",
    "This project demonstrates hyperparameter tuning for a scikit-learn `DecisionTreeClassifier` using [Optuna](https://optuna.org/) and experiment tracking with [MLflow](https://mlflow.org/).\n",
    "\n",
    "## Objective Function\n",
    "\n",
    "The objective function in Optuna:\n",
    "1. Defines the hyperparameter search space.\n",
    "2. Trains the model with suggested hyperparameters.\n",
    "3. Evaluates the modelâ€™s performance.\n",
    "4. Returns a scalar value for Optuna to optimize (minimize or maximize).\n",
    "\n",
    "## Hyperparameters Tuned\n",
    "\n",
    "For the `DecisionTreeClassifier`, the following hyperparameters are tuned:\n",
    "- **criterion**: Chooses between `gini` and `entropy`. This determines the function used to measure the quality of a split.\n",
    "- **max_depth**: Integer between 5 and 50.\n",
    "- **min_samples_split**: Integer between 2 and 40.\n",
    "- **min_samples_leaf**: Integer between 1 and 20.\n",
    "\n",
    "## Optimization Process\n",
    "\n",
    "- The search algorithm can use various samplers (e.g., TPE, GPSampler).\n",
    "- Each Optuna trial starts a new MLflow run for experiment tracking.\n",
    "- Model performance is evaluated using 5-fold cross-validation, and the negative mean of the fold results is used as the optimization target.\n",
    "\n",
    "## Impurity Measures\n",
    "\n",
    "- **Gini impurity**: Measures the likelihood of incorrect classification of a randomly chosen element.\n",
    "- **Entropy**: Measures the impurity or disorder in the dataset.\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. Install dependencies:\n",
    "   ```bash\n",
    "   pip install optuna mlflow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461804c4-ca61-4e70-b01c-c3fc02e5cadc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def optuna_objective_function(trial):\n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 50),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 40),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20)\n",
    "    }\n",
    "\n",
    "    # Start an MLflow run for logging\n",
    "    with mlflow.start_run(nested=True, run_name=f\"Model Tuning with Optuna - Trial {trial.number}\"):\n",
    "        # Log parameters with MLflow\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        dtc = DecisionTreeClassifier(**params)\n",
    "        scoring_metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "        cv_results = cross_validate(dtc, X_train, y_train, cv=5, scoring=scoring_metrics, return_estimator=True)\n",
    "\n",
    "        # Log cross-validation metrics to MLflow\n",
    "        for metric in scoring_metrics:\n",
    "            mlflow.log_metric(f'{metric}', cv_results[f'test_{metric}'].mean())\n",
    "\n",
    "        # Train the model on the full training set\n",
    "        final_model = DecisionTreeClassifier(**params)\n",
    "        final_model.fit(X_train, y_train)\n",
    "\n",
    "        # Create input signature using the first row of X_train\n",
    "        input_example = X_train.iloc[[0]]\n",
    "        signature = infer_signature(input_example, final_model.predict(input_example))\n",
    "\n",
    "        # Log the model with input signature\n",
    "        mlflow.sklearn.log_model(final_model, \"decision_tree_model\", signature=signature, input_example=input_example)\n",
    "\n",
    "        # Compute the mean from cross-validation\n",
    "        f1_score_mean = cv_results['test_f1'].mean()\n",
    "\n",
    "        # Metric to be minimized\n",
    "        return -f1_score_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "670f2a22-eb48-41ea-8d52-d1390397da7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Optimize the Scikit-Learn Model on Single-Machine Optuna and Log Results with MLflow\n",
    "\n",
    "Before running the optimization, we need to perform two key steps:\n",
    "\n",
    "1. **Initialize an Optuna Study using `optuna.create_study()`.**\n",
    "   - A study represents an optimization process consisting of multiple trials.\n",
    "   - A trial is a single execution of the objective function with a specific set of hyperparameters.\n",
    "\n",
    "2. **Run the Optimization using `study.optimize()`.**\n",
    "   - This tells Optuna how many trials to perform and allows it to explore the search space.\n",
    "\n",
    "Each trial will be logged to MLflow, including the hyperparameters tested and their corresponding cross-validation results. Optuna will handle the optimization while training continues.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "- **Set up an Optuna study with `optuna.create_study()`.**\n",
    "- **Start an MLflow run with `mlflow.start_run()` to log experiments.**\n",
    "- **Optimize hyperparameters using `study.optimize()` within the MLflow context.**\n",
    "\n",
    "---\n",
    "\n",
    "### Note on `n_jobs` in `study.optimize()`\n",
    "\n",
    "The `n_jobs` argument controls the **number of trials running in parallel** using multi-threading **on a single machine**.\n",
    "\n",
    "- If `n_jobs=-1`, Optuna will use **all available CPU cores** (e.g., on a 4-core machine, it will likely use all 4 cores).\n",
    "- If `n_jobs` is undefined (default), trials run **sequentially (single-threaded)**.\n",
    "- **Important:** `n_jobs` does **not** distribute trials across multiple nodes in a Spark cluster. To parallelize across nodes, use `SparkTrials()` instead.\n",
    "\n",
    "---\n",
    "\n",
    "### Why We Don't Use `MLflowCallback`\n",
    "\n",
    "Optuna provides an `MLflowCallback` for automatic logging. However, in this demo, we are demonstrating how to integrate the MLflow API with Optuna separate from `MLflowCallback`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e512da4-9961-4491-8ce4-eaeaa750f51e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the MLflow experiment name and get the id\n",
    "experiment_name = \"/Users/sohagahammed.siyam@kone.com/Databricks Training/MLOps/Optuna Experiment\"\n",
    "\n",
    "print(f\"Experiment Name: {experiment_name}\")\n",
    "mlflow.set_experiment(experiment_name)\n",
    "experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "print(f\"Experiment ID: {experiment_id}\")\n",
    "\n",
    "print(\"Clearing out old runs (If you want to add more runs, change the n_trial parameter in the next cell) ...\")\n",
    "# Get all runs\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment_id], output_format=\"pandas\")\n",
    "\n",
    "if runs.empty:\n",
    "    print(\"No runs found in the experiment.\")\n",
    "else:\n",
    "    # Iterate and delete each run\n",
    "    for run_id in runs[\"run_id\"]:\n",
    "        mlflow.delete_run(run_id)\n",
    "        print(f\"Deleted run: {run_id}\")\n",
    "\n",
    "print(\"All runs have been deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97eca4c3-95db-4db7-a7a0-6e1ae573419d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    study_name=\"optuna-hyper-param-optimization\",\n",
    "    direction=\"minimize\"\n",
    ")\n",
    "\n",
    "with mlflow.start_run(run_name='demo_optuna_hpo') as parent_run:\n",
    "    # Run optimization\n",
    "    study.optimize(\n",
    "        optuna_objective_function,\n",
    "        n_trials=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "990538ec-c938-407b-a7b6-a70e39f86024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Review Tuning Results\n",
    "We can use the MLflow API to review the trial results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7465fd0b-b745-42cd-b4f7-4cdda7ac0f5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "# Define your experiment name or ID\n",
    "experiment_id = parent_run.info.experiment_id  # Replace with your actual experiment ID\n",
    "\n",
    "# Fetch all runs from the experiment\n",
    "df_runs = mlflow.search_runs(\n",
    "    experiment_ids=[experiment_id]\n",
    ")\n",
    "\n",
    "# df_runs = df_runs[df_runs['tags.mlflow.runName'] != 'demo_optuna_hpo']\n",
    "\n",
    "display(df_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdffa881-976f-4f8f-b82f-6f5ed4644cdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can get the parameters for the best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dbf8869-39ce-4131-842e-3090ca0d57a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Best Hyperparameters: {study.best_params}\")\n",
    "print(f\"Best negative f1 Score: {study.best_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67e88a7d-09f3-4ab2-a94d-5a6ebdccc15d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Find the Best Run Based on F1-Score\n",
    "\n",
    "In this section, we will search for registered models. There are a couple of ways for achieving this. We will show how to search runs using the MLflow API and the UI.\n",
    "\n",
    "**The output links for using Optuna gave the best runs. Why can't we just use that?**\n",
    "\n",
    "You totally can! But this is the same as using the UI to navigate to the trial that was the best (which is shown below).\n",
    "\n",
    "### Option 1: Find the Best Run - MLflow API\n",
    "\n",
    "Using the MLflow API, you can search runs in an experiment, which returns results into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b7fa036-a763-4c84-b818-e61123a57fcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "experiment_id = parent_run.info.experiment_id\n",
    "print(f\"Experiment ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c30ca92-4885-4fe5-83af-4440a5405907",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "search_runs_pd = mlflow.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    order_by=[\"metrics.cv_f1 DESC\"],\n",
    "    max_results=1,\n",
    "    )\n",
    "\n",
    "display(search_runs_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a228be4-1599-4d70-9bcc-1b2efad47f7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Option 2 - Find the Best Run - MLflow UI\n",
    "\n",
    "The simplest way of seeing the tuning result is to use the MLflow UI.\n",
    "\n",
    "1. Click on **Experiments** from the left menu.\n",
    "2. Select the experiment which has the same name as this notebook's title (**2.1 - Hyperparameter Tuning with Optuna**).\n",
    "3. Click on the graph icon at the top left under **Run**.\n",
    "4. Click on the parent run or manually select all 10 runs to compare. The graphs on the right of the screen will appear for inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a33386b5-e0f3-4d4d-9d1e-baf9e04c777e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Visualize the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a03505af-9a0b-4bf0-875d-b75aa215aa02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_runs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38ec374d-260a-471c-b51e-3c44b860bb4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the necessary parameters exist in the DataFrame before plotting\n",
    "required_params = [\"params.min_samples_leaf\", \"params.max_depth\", \"params.min_samples_split\", \"metrics.f1\", \"tags.mlflow.runName\"]\n",
    "df_filtered = df_runs.dropna(subset=required_params, how=\"any\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd1c3877-20bd-45ee-9ab2-7e8fc58eb6b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convert parameters to appropriate types\n",
    "df_filtered[\"params.min_samples_split\"] = df_filtered[\"params.min_samples_split\"].astype(float)\n",
    "df_filtered[\"params.max_depth\"] = df_filtered[\"params.max_depth\"].astype(float)\n",
    "df_filtered[\"metrics.f1\"] = df_filtered[\"metrics.f1\"].astype(float)\n",
    "\n",
    "# Identify the best run index (assuming higher f1 is better)\n",
    "best_run_index = df_filtered[\"metrics.f1\"].idxmax()\n",
    "best_run_name = df_filtered.loc[best_run_index, \"tags.mlflow.runName\"]\n",
    "\n",
    "# Extract run names for x-axis labels\n",
    "run_names = df_filtered[\"tags.mlflow.runName\"]\n",
    "\n",
    "# Create a figure and axis for bar chart\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Bar chart for min_samples_split and max_depth\n",
    "df_filtered[[\"params.min_samples_split\", \"params.max_depth\"]].plot(kind=\"bar\", ax=ax1, edgecolor=\"black\")\n",
    "\n",
    "ax1.set_xlabel(\"Run Name\")\n",
    "ax1.set_ylabel(\"Parameter Values\")\n",
    "ax1.set_title(\"Hyperparameters & CV f1 Score per Run\")\n",
    "ax1.legend([\"Max Features\", \"Max Depth\"])\n",
    "ax1.set_xticks(range(len(df_filtered)))\n",
    "ax1.set_xticklabels(run_names, rotation=45, ha=\"right\")  # Rotate for readability\n",
    "\n",
    "# Create a second y-axis for the f1 score line chart\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(\n",
    "    range(len(df_filtered)),  # X-axis indices\n",
    "    df_filtered[\"metrics.f1\"],\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    color=\"blue\",\n",
    "    label=\"f1 Score\"\n",
    ")\n",
    "\n",
    "# Highlight the best run with a bold marker\n",
    "ax2.plot(\n",
    "    df_filtered.index.get_loc(best_run_index),  # Get positional index\n",
    "    df_filtered.loc[best_run_index, \"metrics.f1\"],\n",
    "    marker=\"o\",\n",
    "    markersize=10,\n",
    "    color=\"red\",\n",
    "    label=\"Best Run\"\n",
    ")\n",
    "\n",
    "# Add a vertical dashed line to indicate the best run\n",
    "ax2.axvline(df_filtered.index.get_loc(best_run_index), color=\"red\", linestyle=\"--\")\n",
    "\n",
    "ax2.set_ylabel(\"f1 Score\")\n",
    "\n",
    "# Add legend\n",
    "fig.legend(loc=\"upper left\", bbox_to_anchor=(0.1, 0.9))\n",
    "plt.show()\n",
    "\n",
    "# Pie chart for criterion\n",
    "plt.figure(figsize=(8, 8))\n",
    "df_filtered[\"params.criterion\"].value_counts().plot(kind=\"pie\", autopct=\"%.1f%%\", startangle=90)\n",
    "plt.title(\"Criterion Distribution\")\n",
    "plt.ylabel(\"\")  # Hide y-label for better visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a962512-57aa-4637-a4d7-ff7d3f09d228",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load the Best Model and Parameters\n",
    "\n",
    "To load the model and make a prediction, let's use the information from Option 2 shown above. Run the next cell to get the value.\n",
    "\n",
    "### Copy and Paste Option\n",
    "\n",
    "Alternatively, you can set the variables shown below manually. Using either the output from Option 1 or Option 2 or the UI from Option 3, locate the `run_id` and the `experiment_id`. With Option 1 or 2, this is simply the value in the first two columns. In the UI, this is presented to you in the Details table when clicking on the specific run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f7dc6fd-af20-4ca2-bce8-6078141ba277",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert search_runs_pd to pyspark dataframe\n",
    "search_runs_sd = spark.createDataFrame(search_runs_pd)\n",
    "\n",
    "# Get the string value from run_id and experiment_id from PySpark DataFrame hpo_runs_df\n",
    "run_id = search_runs_sd.select(\"run_id\").collect()[0][0]\n",
    "experiment_id = search_runs_sd.select(\"experiment_id\").collect()[0][0]\n",
    "\n",
    "print(f\"Run ID: {run_id}\")\n",
    "print(f\"Experiment ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dec07c2c-a823-4c96-a4e8-24c6f9b9309e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import json\n",
    "from mlflow.models import Model\n",
    "\n",
    "# Grab an input example from the test set\n",
    "input_example = X_test.iloc[[0]]\n",
    "\n",
    "model_path = f\"dbfs:/databricks/mlflow-tracking/{experiment_id}/{run_id}/artifacts/decision_tree_model\"\n",
    "# Load the model using the run ID\n",
    "loaded_model = mlflow.pyfunc.load_model(model_path)\n",
    "\n",
    "# Retrieve model parameters\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "params = client.get_run(run_id).data.params\n",
    "\n",
    "# Display model parameters\n",
    "print(\"Best Model Parameters:\")\n",
    "print(json.dumps(params, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5daa2aad-50dd-4850-8e71-2bd9d966da6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dcfb888-76ee-4e82-ba09-77a901fd05ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Make a prediction\n",
    "test_prediction = loaded_model.predict(input_example)\n",
    "\n",
    "# X_test is a pandas dataframe â€“ let's add the test_prediction output as a new column\n",
    "input_example['prediction'] = test_prediction\n",
    "display(input_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e754db8c-fb7e-411d-82a4-1d8b956dd822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Register the model to Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5575636-17f9-4750-b36e-7b219e69c4fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_uri = f'runs:/{run_id}/decision_tree_model'\n",
    "mlflow.register_model(model_uri=model_uri, name=\"sdd_dev.sohag_test.demo_optuna_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6463be67-f96c-4b72-8484-128dc9200431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this demo, we've explored how to enhance your model's performance using **Optuna** for hyperparameter optimization and **MLflow** for tracking the tuning process. By employing Optuna's efficient search algorithms, you've learned to fine-tune your model's parameters effectively. Simultaneously, MLflow has facilitated seamless monitoring and logging of each trial, capturing essential information such as hyperparameters, metrics, and intermediate results. Additionally, you learned how to register the best model within Unity Catalog. Moving forward, integrating these tools into your workflow will be instrumental in improving your model's performance and simplifying the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afea1f89-784a-4194-9fdc-be29a0d4db4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "5. Hyperparameter Tuning with Optuna",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
