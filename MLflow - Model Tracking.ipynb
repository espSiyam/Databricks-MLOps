{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "521abcf0-d99f-4233-9f5d-fd25f1115026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Model Tracking with MLflow\n",
    "\n",
    "In this demo, we will explore the capabilities of MLflow, a comprehensive framework for the complete machine learning lifecycle. MLflow provides tools for tracking experiments, packaging code into reproducible runs, and sharing and deploying models.\n",
    "\n",
    "In this demo, we will focus on tracking and logging components of MLflow. First, we will demonstrate how to track an experiment with MLflow and show various custom logging features including logging parameters, metrics, figures and arbitrary artifacts.\n",
    "\n",
    "## Learning Objectives:\n",
    "\n",
    "By the end of this demo, you will be able to:\n",
    "\n",
    "* Train a model using a Feature Store table as the modeling set\n",
    "* Manually log parameters, metrics, models, and figures with MLflow tracking\n",
    "* Log training dataset with model in MLflow\n",
    "* Log additional artifacts to a model run\n",
    "* Review an experiment using the MLflow UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdb297b6-b2c4-4cee-ab42-b0048f7b3318",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"sdd_dev.sohag_test.diabetes_binary_health_indicators_brfss_2015\"\n",
    "feature_dataset = spark.read.table(table_name)\n",
    "feature_data_pd = feature_dataset.toPandas()\n",
    "feature_data_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fe7d17c-3c89-483d-8577-4cf8191c91a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert all columns in the Dataframe to double data type\n",
    "for column in feature_data_pd.columns:\n",
    "    feature_data_pd[column] = feature_data_pd[column].astype(float)\n",
    "\n",
    "print(feature_data_pd.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fac8cc9f-6541-410e-92da-6fcd3fa44db0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the target variable into it's own dataset\n",
    "target_col = \"Diabetes_binary\"\n",
    "\n",
    "X_all = feature_data_pd.drop(target_col, axis=1)\n",
    "y_all = feature_data_pd[target_col]\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=0)\n",
    "\n",
    "print(f\"We have {len(X_train)} rows in the training set and {len(X_test)} rows in the test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45001281-4231-4d1f-9d29-ca0cf3ebc25b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MLflow Parameter Logging\n",
    "\n",
    "In this code, we use MLflow to start a run and log parameters such as the criterion and max_depth of the Decision Tree model. After fitting the model on the training data, we evaluate its performance on the test set and log the accuracy as a metric.\n",
    "\n",
    "### Important Notes:\n",
    "- **MLflow autologging is enabled by default on Databricks**. This means you don't need to do anything for supported libraries. In the next section, we are disabling it and manually log params, metrics etc. just demonstrate how to do it manually when you need to log any custom model info.\n",
    "- **Note**: We won't define the `experiment_name`, all runs generated in this notebook will be logged under the notebook title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d0d2da7-27f3-444d-b7ad-2ddd6eeb01c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dtc_params = {\n",
    "    'criterion': 'gini',\n",
    "    'max_depth': 10,\n",
    "    'min_samples_split': 20,\n",
    "    'min_samples_leaf': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b131d849-5508-441c-85e0-4ddf6c93c49a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import *\n",
    "\n",
    "UC_PATH = \"databricks-uc\"\n",
    "EXP_PATH = \"/Users/sohagahammed.siyam@kone.com/Databricks Training/MLOps/MLflow - Model Tracking\"\n",
    "MODEL_NAME = \"sdd_dev.sohag_test.diabetes_prediction\"\n",
    "\n",
    "# register models in UC and Notebook\n",
    "mlflow.set_registry_uri(UC_PATH)\n",
    "\n",
    "# Create experiment if it does not exist\n",
    "experiment = mlflow.get_experiment_by_name(EXP_PATH)\n",
    "if experiment is None:\n",
    "    mlflow.create_experiment(EXP_PATH)\n",
    "\n",
    "mlflow.set_experiment(EXP_PATH)\n",
    "\n",
    "# Turn of autologging\n",
    "mlflow.sklearn.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "469bcd71-e0c0-4fb4-8252-540c455895dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# start an MLFlow run\n",
    "with mlflow.start_run(run_name=\"Model tracking demo\") as run:\n",
    "    # Log the dataset as artifacts\n",
    "    feature_dataset.toPandas().to_csv(\"/dbfs/tmp/feature_dataset.csv\", index=False)\n",
    "    X_train.to_csv(\"/dbfs/tmp/X_train.csv\", index=False)\n",
    "    X_test.to_csv(\"/dbfs/tmp/X_test.csv\", index=False)\n",
    "    \n",
    "    mlflow.log_artifact(\"/dbfs/tmp/feature_dataset.csv\", artifact_path=\"datasets/source\")\n",
    "    mlflow.log_artifact(\"/dbfs/tmp/X_train.csv\", artifact_path=\"datasets/train\")\n",
    "    mlflow.log_artifact(\"/dbfs/tmp/X_test.csv\", artifact_path=\"datasets/test\")\n",
    "    \n",
    "    # log our parameters\n",
    "    mlflow.log_params(dtc_params)\n",
    "    \n",
    "    # fit the model\n",
    "    dtc = DecisionTreeClassifier(**dtc_params)\n",
    "    dtc_mdl = dtc.fit(X_train, y_train)\n",
    "\n",
    "    # Define model signature\n",
    "    signature = infer_signature(X_all, y_all)\n",
    "    \n",
    "    # log the model\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=dtc_mdl,\n",
    "        artifact_path=\"model-artifact\",\n",
    "        signature=signature,\n",
    "        registered_model_name=MODEL_NAME\n",
    "    )\n",
    "\n",
    "    # Evaluate on the training set\n",
    "    y_pred = dtc_mdl.predict(X_train)\n",
    "\n",
    "    # Log metrics like accuracy, precision, recall, f1\n",
    "    mlflow.log_metric(\"train_accuracy\", accuracy_score(y_train, y_pred))\n",
    "    mlflow.log_metric(\"train_precision\", precision_score(y_train, y_pred))\n",
    "    mlflow.log_metric(\"train_recall\", recall_score(y_train, y_pred))\n",
    "    mlflow.log_metric(\"train_f1\", f1_score(y_train, y_pred))\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    y_pred = dtc_mdl.predict(X_test)\n",
    "    mlflow.log_metric(\"test_accuracy\", accuracy_score(y_test, y_pred))\n",
    "    mlflow.log_metric(\"test_precision\", precision_score(y_test, y_pred))\n",
    "    mlflow.log_metric(\"test_recall\", recall_score(y_test, y_pred))\n",
    "    mlflow.log_metric(\"test_f1\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72e158c6-ac2d-41ed-8a5a-35ee682d7ab8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Log model Artifacts\n",
    "In addition to logging the model, we can also log other artifacts such as the training dataset, feature dataset, and model run metadata. Let's setup an MLFlow client to log artifacts after the run is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e036106-09ca-4d7e-af27-91a70335ff6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77d3dd00-f7c8-4563-90bb-50d17a1b82bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.client import MlflowClient\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# Log confusion matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
    "ax.set_title(\"Confusion Matrix\")\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"Actual\")\n",
    "# mlflow.log_figure(fig, \"confusion_matrix.png\")\n",
    "\n",
    "client.log_figure(run.info.run_id, artifact_file=\"confusion_matrix.png\", figure=fig)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MLflow - Model Tracking",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
