{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "574a4c77-fede-4aec-a066-78903f045c30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Batch Deployment\n",
    "\n",
    "Batch inference is the most common way of deploying machine learning models. This lesson introduces various strategies for deploying models using batch including Spark. In addition, we will show how to enable optimizations for Delta tables.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "**By the end of this demo, you will be able to:**\n",
    "\n",
    "- Load a logged Model Registry model using `pyfunc`.\n",
    "- Compute predictions using `pyfunc` APIs.\n",
    "- Perform batch inference using Feature Engineering's `score_batch` method.\n",
    "- Materialize predictions into inference tables (Delta Lake).\n",
    "- Perform common write optimizations like liquid clustering, predictive optimization to maximize data skipping and on inference tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04f5799e-5a63-4644-800f-d51eaa5c8ecd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For this demonstration, we will utilize a fictional dataset from a Telecom Company, which includes customer information. This dataset encompasses **customer demographics**, including gender, as well as internet subscription details such as subscription plans and payment methods.\n",
    "\n",
    "After loading the dataset, we will perform simple **data cleaning and feature selection**.\n",
    "\n",
    "In the final step, we will split the dataset into **features** and **response** sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76d048ae-fb80-498a-a007-54198c596b20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f56d7c0f-7da8-4c18-b93b-c4a9fd983ee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# dataset path (Delta table)\n",
    "dataset_p_telco = \"sdd_dev.sohag_test.telco_customer_churn\"\n",
    "\n",
    "# features to use\n",
    "primary_key = \"customerID\"\n",
    "response = \"Churn\"\n",
    "features = [\"SeniorCitizen\", \"tenure\", \"MonthlyCharges\", \"TotalCharges\"]  # Keeping numerical only for simplicity and demo purposes\n",
    "\n",
    "# Read dataset (and drop nan)\n",
    "telco_df = spark.read.table(dataset_p_telco) \\\n",
    "    .withColumn(\"TotalCharges\", col(\"TotalCharges\").cast(\"double\")) \\\n",
    "    .withColumn(\"SeniorCitizen\", col(\"SeniorCitizen\").cast(\"double\")) \\\n",
    "    .withColumn(\"tenure\", col(\"tenure\").cast(\"double\")) \\\n",
    "    .na.drop(how=\"any\")\n",
    "\n",
    "# Split with 80 percent of the data in train_df and 20 percent of the data in test_df\n",
    "train_df, test_df = telco_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Separate features and ground-truth\n",
    "features_df = train_df.select(primary_key, *features)\n",
    "response_df = train_df.select(primary_key, response)\n",
    "\n",
    "# review the features dataset\n",
    "display(features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f182bf9d-f471-4c00-be83-ff1679487958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Batch Deployment - Without Feature Store\n",
    "\n",
    "This demo will cover two main batch deployment methods. The first method is deploying models without a feature table. For the second method, we will use a feature table to train the model and later use the feature table for inference.\n",
    "\n",
    "## Setup Model Registry with UC\n",
    "\n",
    "Before we start model deployment, we need to fit and register a model. In this demo, **we will log models to Unity Catalog**, which means first we need to setup the **MLflow Model Registery URI**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceb3c27f-b2df-4f38-b39c-985a41557b13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Point to UC model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = mlflow.MlflowClient()\n",
    "\n",
    "# helper function that we will use for getting latest version of a model\n",
    "def get_latest_model_version(model_name):\n",
    "    \"\"\"Helper function to get latest model version\"\"\"\n",
    "    model_version_infos = client.search_model_versions(\"name = '%s'\" % model_name)\n",
    "    return max([model_version_info.version for model_version_info in model_version_infos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75dc4c37-7796-4ce3-88a9-af685aaedf57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Fit and Register a Model with UC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e26c762-6ad0-4ae5-84fc-09b31253e51d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train a sklearn Decision Tree Classification model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Convert data to pandas dataframes\n",
    "X_train_pdf = features_df.drop(primary_key).toPandas()\n",
    "Y_train_pdf = response_df.drop(primary_key).toPandas()\n",
    "clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "\n",
    "# Use 3-level namespace for model name\n",
    "model_name = \"sdd_dev.sohag_test.churn_ml_model\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"Model-Batch-Deployment-Demo\") as mlflow_run:\n",
    "    # Enable automatic logging of input samples, metrics, parameters, and models\n",
    "    mlflow.sklearn.autolog(\n",
    "        log_input_examples=True,\n",
    "        log_models=False,\n",
    "        log_post_training_metrics=True,\n",
    "        silent=True\n",
    "    )\n",
    "\n",
    "    clf.fit(X_train_pdf, Y_train_pdf)\n",
    "\n",
    "    # Log model and push to registry\n",
    "    signature = infer_signature(X_train_pdf, Y_train_pdf)\n",
    "    mlflow.sklearn.log_model(\n",
    "        clf,\n",
    "        artifact_path=\"decision_tree\",\n",
    "        signature=signature,\n",
    "        registered_model_name=model_name\n",
    "    )\n",
    "\n",
    "# Set model alias (i.e. Baseline)\n",
    "client.set_registered_model_alias(model_name, \"Baseline\", get_latest_model_version(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c7197d6-f73d-4fea-a784-d093edb77e0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Use the Model for Inference\n",
    "Now that our model is ready in model registry, we can use it for inference. In this section we will use the model for inference directly on a spark dataframe, which is called **batch inference**.\n",
    "\n",
    "\n",
    "### Load the Model\n",
    "Loading a model from UC-based model registry is done by getting a model using **alias** and **version**.\n",
    "\n",
    "After loading the model, we will create a `spark_udf` from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8effa950-01ac-4bac-a96a-33e42908d887",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "latest_model_version = client.get_model_version_by_alias(name=model_name, alias=\"baseline\").version\n",
    "model_uri = f\"models:/{model_name}/{latest_model_version}\"  # Should be version 1\n",
    "# model_uri = f\"models:/{model_name}@baseline\"  # uri can also point to @alias\n",
    "predict_func = mlflow.pyfunc.spark_udf(\n",
    "    spark,\n",
    "    model_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5030b1d2-fb1a-4882-bb9a-419075b85620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Infernece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a731704-4f9b-4d65-abf1-34c9d6f04e1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prepare test dataset\n",
    "test_features_df = test_df.select(primary_key, *features)\n",
    "\n",
    "# make prediction\n",
    "prediction_df = test_features_df.withColumn(\n",
    "    \"prediction\",\n",
    "    predict_func(*test_features_df.drop(primary_key).columns)\n",
    ")\n",
    "\n",
    "display(prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2af4a724-96ef-4b00-83f0-86cd60fbf730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_features_df.drop(primary_key).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b222fa6-f212-4d1f-9811-97de6d7c77df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Batch Deployment - With Feature Store\n",
    "In the previous section we trained and registered a model using Spark dataframe. In some cases, you will need to use features from a feature store for training and inference.\n",
    "\n",
    "In this section we will demonstrate how to train and deploy a model using Feature Store.\n",
    "\n",
    "\n",
    "## Create Feature Table\n",
    "Let's create a feature table based on the `features_df` that we created before. Please note that we will be using **Feature Store with Unity Catalog**, which means we need to use `FeatureEngineeringClient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2852bf15-5dce-44fd-88ac-6209a0df650a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "# prepare feature set\n",
    "features_df_all = telco_df.select(primary_key, *features)\n",
    "\n",
    "# feature table definition\n",
    "fe = FeatureEngineeringClient()\n",
    "feature_table_name = \"sdd_dev.sohag_test.telco_customer_churn_features\"\n",
    "\n",
    "# drop table if exists\n",
    "try:\n",
    "    fe.drop_table(name=feature_table_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create feature table\n",
    "fe.create_table(\n",
    "    name=feature_table_name,\n",
    "    df=features_df_all,\n",
    "    primary_keys=[primary_key],\n",
    "    description=\"Example feature table\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6bbeace-4687-4ce3-9e5b-febb9b5d5e8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup Feature Lookups\n",
    "\n",
    "In order to create a training set from the feature table, we need to define a **feature lookup**. This will be used for creating a training set from the feature table.\n",
    "\n",
    "Note that the `lookup_key` is used for matching records in the feature table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f61c646-03b2-4173-b70d-ff07d253e361",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create training set based on feature lookup\n",
    "from databricks.feature_engineering import FeatureLookup\n",
    "\n",
    "fl_handle = FeatureLookup(\n",
    "    table_name=feature_table_name,\n",
    "    lookup_key=[primary_key]\n",
    ")\n",
    "\n",
    "training_set_spec = fe.create_training_set(\n",
    "    df=response_df,\n",
    "    label=response,\n",
    "    feature_lookups=[fl_handle],\n",
    "    exclude_columns=[primary_key]\n",
    ")\n",
    "\n",
    "# Load training dataframe based on defined feature-lookup specification\n",
    "training_df = training_set_spec.load_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c25bea4-f485-4123-9756-a86b1d8dd95e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c71b8a9-69ce-4640-aa80-cf0a4587479d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Fit and Register a Model with UC using Feature Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa038ea1-4bdb-4bc9-be4a-9defc4a5115a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train a sklearn Decision Tree Classification model\n",
    "import warnings\n",
    "from mlflow.types.utils import _infer_schema\n",
    "\n",
    "# Convert data to pandas dataframes\n",
    "X_train_pdf2 = training_df.drop(primary_key, response).toPandas()\n",
    "Y_train_pdf2 = training_df.select(response).toPandas()\n",
    "clf2 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Model-Batch-Deployment-Demo-With-FS\") as mlflow_run:\n",
    "    # Enable automatic logging of input samples, metrics, parameters, and models\n",
    "    mlflow.sklearn.autolog(\n",
    "        log_input_examples=True,\n",
    "        log_models=False,\n",
    "        log_post_training_metrics=True,\n",
    "        silent=True\n",
    "    )\n",
    "\n",
    "    clf2.fit(X_train_pdf2, Y_train_pdf2)\n",
    "\n",
    "    # Infer output schema\n",
    "    try:\n",
    "        output_schema = _infer_schema(Y_train_pdf2)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Could not infer model output schema: {e}\")\n",
    "        output_schema = None\n",
    "\n",
    "    # Log using feature engineering client and push to registry\n",
    "    fe.log_model(\n",
    "        model=clf2,\n",
    "        artifact_path=\"decision_tree\",\n",
    "        flavor=mlflow.sklearn,\n",
    "        training_set=training_set_spec,\n",
    "        output_schema=output_schema,\n",
    "        registered_model_name=model_name\n",
    "    )\n",
    "\n",
    "# Set model alias (i.e. Champion)\n",
    "client.set_registered_model_alias(model_name, \"Champion\", get_latest_model_version(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dd4782a-4d83-4123-92c7-684e454ea9d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcb6dbbc-ec99-466e-92a8-1bdaf4082ac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Use the Model for Inference\n",
    "\n",
    "Inference for models that are registered with a Feature Store table is different than inference with Spark dataframe. For inference, we will use feature engineering client's `.score_batch()` method. This method takes a model URI and dataframe with primary key info.\n",
    "\n",
    "**So how does the function know which feature table to use?**  \n",
    "If you visit the Artifacts section of the registered model, you will see a `data` folder is registered with the model. Also, the model file includes `data: data/feature_store` statement to define feature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7655aa36-2e4a-4e8a-b8ef-7f5032e240b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "champion_model_uri = f\"models:/{model_name}@champion\"\n",
    "\n",
    "# prepare lookup dataset\n",
    "lookup_df = test_df.select(\"customerID\")\n",
    "\n",
    "# predict in batch using lookup df\n",
    "prediction_fe_df = fe.score_batch(\n",
    "    model_uri=champion_model_uri,\n",
    "    df=lookup_df,\n",
    "    result_type='string'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66152d71-951b-40b8-8a6a-fa00505dcb52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(prediction_fe_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11b95922-028c-46ab-afc7-d305535dcdbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Performance Considerations\n",
    "\n",
    "There are many possible (write) optimizations that Delta Lake can offer such as:\n",
    "- **Partitioning:** stores data associated with different categorical values in different directories.\n",
    "- **Z-Ordering:** colocates related information in the same set of files.\n",
    "- **Liquid Clustering:** replaces both above-mentioned methods to simplify data layout decisions and optimize query performance.\n",
    "- **Predictive Optimizations:** removes the need to manually manage maintenance operations for Delta tables on Databricks.\n",
    "\n",
    "In this demo, we will show the last two options: liquid clustering and predictive optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d361e6c9-fee1-4202-9149-a24febbdeefe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = \"sdd_dev\"\n",
    "schema_name = \"sohag_test\"\n",
    "table_name = \"telco_customer_churn\"\n",
    "\n",
    "# Set catalog and schema\n",
    "spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98f05c3d-6971-4edd-81d9-f245980ef115",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Enable Predictive Optimization at schema level (can also be done at catalog level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50b086a3-3e8f-4e50-9ffc-33d7927fc1a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"ALTER SCHEMA {catalog_name}.{schema_name} ENABLE PREDICTIVE OPTIMIZATION;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5af155fb-0e0a-4ae8-9eb8-fcbb834d1f43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create inference table (where batch scoring jobs would materialize) and enable liquid clustering using `CLUSTER BY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c097e823-9d69-4172-95db-a700f3a44ed2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create or replace the batch_inference table, clustered by customerID and tenure\n",
    "CREATE OR REPLACE TABLE batch_inference(\n",
    "    customerID STRING,\n",
    "    Churn STRING,\n",
    "    SeniorCitizen DOUBLE,\n",
    "    tenure DOUBLE,\n",
    "    MonthlyCharges DOUBLE,\n",
    "    TotalCharges DOUBLE,\n",
    "    prediction STRING\n",
    ")\n",
    "CLUSTER BY (customerID, tenure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b076c04-0c66-4083-834c-28db4872ae41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the prediction DataFrame to the batch_inference table\n",
    "prediction_fe_df.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", True) \\\n",
    "    .saveAsTable(f\"{catalog_name}.{schema_name}.batch_inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "777e07bb-75f3-41fa-8541-2ab8ac5ba789",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select *\n",
    "from batch_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11c72d8a-87db-4f75-b0b4-bc60b88755ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6612955645683995,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1. Batch Deployment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
