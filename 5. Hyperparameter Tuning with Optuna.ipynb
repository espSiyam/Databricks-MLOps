{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "748e787a-2757-43b8-9653-c8732f10e484",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Hyperparameter Tuning with Optuna\n",
    "\n",
    "In this hands-on demo, you will learn how to leverage **Optuna**, a powerful optimization library, for efficient model tuning. We'll guide you through the process of performing **hyperparameter optimization**, demonstrating how to define the search space, objective function, and algorithm selection. Throughout the demo, you will utilize _MLflow_ to seamlessly track the model tuning process, capturing essential information such as hyperparameters, metrics, and intermediate results. By the end of the session, you will not only grasp the principles of hyperparameter optimization but also be proficient in finding the best-tuned model using various methods such as the **MLflow API** and **MLflow UI**.\n",
    "\n",
    "By integrating Optuna and MLflow, you can efficiently optimize hyperparameters and maintain comprehensive records of your machine learning experiments, facilitating reproducibility and collaborative research.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "**By the end of this demo, you will be able to:**\n",
    "\n",
    "- Perform hyperparameter optimization using Optuna.\n",
    "- Track the model tuning process with MLflow.\n",
    "- Query previous runs from an experiment using the `MLflowClient`.\n",
    "- Review an MLflow Experiment for visualizing results and selecting the best run.\n",
    "- Read in the best model, make a prediction, and register the model to Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1f19506-9698-45c6-86da-56f78a9c42ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a407e2a0-1c2e-47a3-bd2d-d81407983c03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Installing the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96af9d2b-f3a9-484f-81ef-4e7570a9ca8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qq optuna\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e69ea685-f709-4a6c-9825-6fc3a0d65477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from mlflow.models.signature import infer_signature\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4055452-8bd4-4f17-859d-e0c1509f2859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"sdd_dev.sohag_test.diabetes_binary_health_indicators_brfss_2015\"\n",
    "diabetes_dataset = spark.read.table(table_name)\n",
    "diabetes_pd = diabetes_dataset.toPandas()\n",
    "diabetes_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a74708e-4a61-4254-9b34-45bf0598838e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spliting Train/ Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59cd4fea-b27e-4697-913e-4867dce67364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"We have {diabetes_pd.shape[0]} records in our source dataset\")\n",
    "\n",
    "# split target variable into its own dataset\n",
    "target_col = \"Diabetes_binary\"\n",
    "X_all = diabetes_pd.drop(labels=target_col, axis=1)\n",
    "y_all = diabetes_pd[target_col]\n",
    "\n",
    "# test / train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, train_size=0.95, random_state=42)\n",
    "\n",
    "y_train = y_train.astype(float)\n",
    "y_test = y_test.astype(float)\n",
    "\n",
    "print(f\"We have {X_train.shape[0]} records in our training dataset\")\n",
    "print(f\"We have {X_test.shape[0]} records in our test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7353e924-320e-4744-820f-9987ed1d48f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Hyperparameter Tuning with Optuna and MLflow\n",
    "\n",
    "This project demonstrates hyperparameter tuning for a scikit-learn `DecisionTreeClassifier` using [Optuna](https://optuna.org/) and experiment tracking with [MLflow](https://mlflow.org/).\n",
    "\n",
    "## Objective Function\n",
    "\n",
    "The objective function in Optuna:\n",
    "1. Defines the hyperparameter search space.\n",
    "2. Trains the model with suggested hyperparameters.\n",
    "3. Evaluates the modelâ€™s performance.\n",
    "4. Returns a scalar value for Optuna to optimize (minimize or maximize).\n",
    "\n",
    "## Hyperparameters Tuned\n",
    "\n",
    "For the `DecisionTreeClassifier`, the following hyperparameters are tuned:\n",
    "- **criterion**: Chooses between `gini` and `entropy`. This determines the function used to measure the quality of a split.\n",
    "- **max_depth**: Integer between 5 and 50.\n",
    "- **min_samples_split**: Integer between 2 and 40.\n",
    "- **min_samples_leaf**: Integer between 1 and 20.\n",
    "\n",
    "## Optimization Process\n",
    "\n",
    "- The search algorithm can use various samplers (e.g., TPE, GPSampler).\n",
    "- Each Optuna trial starts a new MLflow run for experiment tracking.\n",
    "- Model performance is evaluated using 5-fold cross-validation, and the negative mean of the fold results is used as the optimization target.\n",
    "\n",
    "## Impurity Measures\n",
    "\n",
    "- **Gini impurity**: Measures the likelihood of incorrect classification of a randomly chosen element.\n",
    "- **Entropy**: Measures the impurity or disorder in the dataset.\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. Install dependencies:\n",
    "   ```bash\n",
    "   pip install optuna mlflow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461804c4-ca61-4e70-b01c-c3fc02e5cadc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def optuna_objective_function(trial):\n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 50),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 40),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20)\n",
    "    }\n",
    "\n",
    "    # Start an MLflow run for logging\n",
    "    with mlflow.start_run(nested=True, run_name=f\"Model Tuning with Optuna - Trial {trial.number}\"):\n",
    "        # Log parameters with MLflow\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        dtc = DecisionTreeClassifier(**params)\n",
    "        scoring_metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "        cv_results = cross_validate(dtc, X_train, y_train, cv=5, scoring=scoring_metrics, return_estimator=True)\n",
    "\n",
    "        # Log cross-validation metrics to MLflow\n",
    "        for metric in scoring_metrics:\n",
    "            mlflow.log_metric(f'{metric}', cv_results[f'test_{metric}'].mean())\n",
    "\n",
    "        # Train the model on the full training set\n",
    "        final_model = DecisionTreeClassifier(**params)\n",
    "        final_model.fit(X_train, y_train)\n",
    "\n",
    "        # Create input signature using the first row of X_train\n",
    "        input_example = X_train.iloc[[0]]\n",
    "        signature = infer_signature(input_example, final_model.predict(input_example))\n",
    "\n",
    "        # Log the model with input signature\n",
    "        mlflow.sklearn.log_model(final_model, \"decision_tree_model\", signature=signature, input_example=input_example)\n",
    "\n",
    "        # Compute the mean from cross-validation\n",
    "        f1_score_mean = cv_results['test_f1'].mean()\n",
    "\n",
    "        # Metric to be minimized\n",
    "        return -f1_score_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "670f2a22-eb48-41ea-8d52-d1390397da7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Optimize the Scikit-Learn Model on Single-Machine Optuna and Log Results with MLflow\n",
    "\n",
    "Before running the optimization, we need to perform two key steps:\n",
    "\n",
    "1. **Initialize an Optuna Study using `optuna.create_study()`.**\n",
    "   - A study represents an optimization process consisting of multiple trials.\n",
    "   - A trial is a single execution of the objective function with a specific set of hyperparameters.\n",
    "\n",
    "2. **Run the Optimization using `study.optimize()`.**\n",
    "   - This tells Optuna how many trials to perform and allows it to explore the search space.\n",
    "\n",
    "Each trial will be logged to MLflow, including the hyperparameters tested and their corresponding cross-validation results. Optuna will handle the optimization while training continues.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "- **Set up an Optuna study with `optuna.create_study()`.**\n",
    "- **Start an MLflow run with `mlflow.start_run()` to log experiments.**\n",
    "- **Optimize hyperparameters using `study.optimize()` within the MLflow context.**\n",
    "\n",
    "---\n",
    "\n",
    "### Note on `n_jobs` in `study.optimize()`\n",
    "\n",
    "The `n_jobs` argument controls the **number of trials running in parallel** using multi-threading **on a single machine**.\n",
    "\n",
    "- If `n_jobs=-1`, Optuna will use **all available CPU cores** (e.g., on a 4-core machine, it will likely use all 4 cores).\n",
    "- If `n_jobs` is undefined (default), trials run **sequentially (single-threaded)**.\n",
    "- **Important:** `n_jobs` does **not** distribute trials across multiple nodes in a Spark cluster. To parallelize across nodes, use `SparkTrials()` instead.\n",
    "\n",
    "---\n",
    "\n",
    "### Why We Don't Use `MLflowCallback`\n",
    "\n",
    "Optuna provides an `MLflowCallback` for automatic logging. However, in this demo, we are demonstrating how to integrate the MLflow API with Optuna separate from `MLflowCallback`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e512da4-9961-4491-8ce4-eaeaa750f51e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the MLflow experiment name and get the id\n",
    "experiment_name = \"/Users/sohagahammed.siyam@kone.com/Databricks Training/MLOps/Optuna Experiment\"\n",
    "\n",
    "print(f\"Experiment Name: {experiment_name}\")\n",
    "mlflow.set_experiment(experiment_name)\n",
    "experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "print(f\"Experiment ID: {experiment_id}\")\n",
    "\n",
    "print(\"Clearing out old runs (If you want to add more runs, change the n_trial parameter in the next cell) ...\")\n",
    "# Get all runs\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment_id], output_format=\"pandas\")\n",
    "\n",
    "if runs.empty:\n",
    "    print(\"No runs found in the experiment.\")\n",
    "else:\n",
    "    # Iterate and delete each run\n",
    "    for run_id in runs[\"run_id\"]:\n",
    "        mlflow.delete_run(run_id)\n",
    "        print(f\"Deleted run: {run_id}\")\n",
    "\n",
    "print(\"All runs have been deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97eca4c3-95db-4db7-a7a0-6e1ae573419d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    study_name=\"optuna-hyper-param-optimization\",\n",
    "    direction=\"minimize\"\n",
    ")\n",
    "\n",
    "with mlflow.start_run(run_name='demo_optuna_hpo') as parent_run:\n",
    "    # Run optimization\n",
    "    study.optimize(\n",
    "        optuna_objective_function,\n",
    "        n_trials=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "990538ec-c938-407b-a7b6-a70e39f86024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Review Tuning Results\n",
    "We can use the MLflow API to review the trial results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7465fd0b-b745-42cd-b4f7-4cdda7ac0f5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "# Define your experiment name or ID\n",
    "experiment_id = parent_run.info.experiment_id  # Replace with your actual experiment ID\n",
    "\n",
    "# Fetch all runs from the experiment\n",
    "df_runs = mlflow.search_runs(\n",
    "    experiment_ids=[experiment_id]\n",
    ")\n",
    "\n",
    "# df_runs = df_runs[df_runs['tags.mlflow.runName'] != 'demo_optuna_hpo']\n",
    "\n",
    "display(df_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dbf8869-39ce-4131-842e-3090ca0d57a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "5. Hyperparameter Tuning with Optuna",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
